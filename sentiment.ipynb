{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,Input,Bidirectional,GRU,LayerNormalization\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/sentiment-cleaned/sentipreprocessed.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.rename(columns={\"Review_body\":\"review\",\"Label\":\"label\"},inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=df.dropna(axis=0).reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=df[:3500000]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"label\"].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=33250,  # use only top 33250 most common words\n                      oov_token='<OOV>') # i done this manually and found that word at index from top (33250) is coming 50 times in whole corpora","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.fit_on_texts(df['review'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(tokenizer.word_index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"review_sequences=tokenizer.texts_to_sequences(df['review'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_len=0\nfor i in range(len(review_sequences)):\n  max_len=max(max_len,len(review_sequences[i]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"label\"]=df[\"label\"]-1;\nreview_labels=np.array(df[\"label\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_ds = tf.data.Dataset.from_tensor_slices((padded_seq[:500000], review_labels[:500000]))\n# train_ds = train_ds.shuffle(1024).batch(256).prefetch(tf.data.AUTOTUNE)\n# val_ds = tf.data.Dataset.from_tensor_slices((padded_seq[1500000:1600000], review_labels[1500000:1600000]))\n# val_ds = val_ds.batch(256).prefetch(tf.data.AUTOTUNE)\nAUTOTUNE = tf.data.AUTOTUNE\n\n# Create the training dataset\ntrain_ds = (\n    tf.data.Dataset.from_tensor_slices((padded_seq[:3000000], review_labels[:3000000]))  # Use full training data\n    .cache()                             # Keep data in memory after first epoch\n    .shuffle(buffer_size=10000)          # Reasonable shuffle buffer\n    .batch(256, drop_remainder=True)     # Drop last batch for shape consistency\n    .prefetch(buffer_size=AUTOTUNE)      # Pipeline the data loading\n)\n\n# Create the validation dataset\nval_ds = (\n    tf.data.Dataset.from_tensor_slices((padded_seq[3000000:3400000], review_labels[3000000:3400000]))\n    .batch(256)\n    .cache()                             # Cache validation set\n    .prefetch(buffer_size=AUTOTUNE)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, BatchNormalization, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Hyperparameters\nVOCAB_SIZE       = 33250\nEMBED_DIM        = 128\nMAX_SEQUENCE_LEN = 208\nLSTM_UNITS       = 128\nDROPOUT_RATE     = 0.4\n\n# Build model\nmodel = Sequential([\n    Embedding(input_dim=VOCAB_SIZE,\n              output_dim=EMBED_DIM,\n              input_length=MAX_SEQUENCE_LEN,\n              mask_zero=True,\n              name=\"embedding\"),\n\n    Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2), name=\"bilstm_1\"),\n    BatchNormalization(name=\"bn_1\"),\n    Dropout(DROPOUT_RATE, name=\"dropout_1\"),\n\n    Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2), name=\"bilstm_2\"),\n    BatchNormalization(name=\"bn_2\"),\n    Dropout(DROPOUT_RATE, name=\"dropout_2\"),\n\n    Bidirectional(LSTM(LSTM_UNITS, return_sequences=False, dropout=0.2), name=\"bilstm_3\"),\n    BatchNormalization(name=\"bn_3\"),\n    Dropout(DROPOUT_RATE, name=\"dropout_3\"),\n\n    Dense(64, activation=\"relu\", name=\"fc1\"),\n    BatchNormalization(name=\"bn_fc1\"),\n    Dropout(0.3, name=\"dropout_fc1\"),\n\n    Dense(1, activation=\"sigmoid\", name=\"classifier\")\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\", tf.keras.metrics.AUC(name=\"auc\")]\n)\n\n# Summary (optional)\nmodel.summary()\n\n# --- Callbacks ---\ncallbacks = [\n    EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True),\n    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5)\n]\n\n# --- Training ---\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=10,\n    callbacks=callbacks\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"sentiment_bilstm_model.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Step 1: Convert predicted probabilities to binary labels using the best threshold\ny_pred_binary = (y_pred_prob >= 0.43).astype(int)\n\n# Step 2: Compute confusion matrix\ncm = confusion_matrix(y_val, y_pred_binary)\n\n# Step 3: Plot confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\ndisp.plot(cmap=\"Blues\", values_format=\"d\")\nplt.title(f\"Confusion Matrix (Threshold = 0.43)\")\nplt.grid(False)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate and print classification report\nreport = classification_report(y_val, y_pred_binary, target_names=[\"Negative\", \"Positive\"])\nprint(report)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Predict probabilities for the positive class\ny_pred_prob = model.predict(X_val, batch_size=512)\n\n# Compute false positive rate, true positive rate, thresholds\nfpr, tpr, thresholds = roc_curve(y_val, y_pred_prob)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC Curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random guess line\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}